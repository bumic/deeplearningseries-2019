{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll train a simple neural network to predict flower species using the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris). This dataset contains the petal and sepal measurements of 150 flowers of three different types: Iris Setosa, Iris Versicolor, and Iris Virginica (there are 50 of each type). Each flower has four associated measurements: sepal length, sepal width, petal length, and petal width (in cm).\n",
    "\n",
    "We'll use [one-hot encoding](https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179) to enocde classes. Basically it works like this: If there are 3 possible classes, and a certain example belongs in the first class, its y-value will be represented as `[1,0,0]`. Second class is `[0,1,0]`, and so on.\n",
    "\n",
    "Each example will have 4 inputs (the four measurements for each flower) and 3 outputs. Each output is the probability that the flower belongs to each class.\n",
    "\n",
    "The network will have one hidden layer.\n",
    "\n",
    "![network](https://cdn-images-1.medium.com/max/1200/1*QVIyc5HnGDWTNX3m-nIm9w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The `X` variable is a 4 x 150 array, where each column represents one flower example.\n",
    "The $i$th column looks like $\\begin{bmatrix}\n",
    "    x^i_{\\textrm{sepal length}} \\\\\n",
    "    x^i_{\\textrm{sepal width}} \\\\\n",
    "    x^i_{\\textrm{petal length}} \\\\\n",
    "    x^i_{\\textrm{petal width}}\n",
    "\\end{bmatrix}$\n",
    "where each entry is the measurement.\n",
    "\n",
    "The `y` variable is a 3 x 150 array.\n",
    "The $i$th column looks like $\\begin{bmatrix}\n",
    "    y^i_{\\textrm{setosa}} \\\\\n",
    "    y^i_{\\textrm{versicolor}} \\\\\n",
    "    y^i_{\\textrm{virginica}} \n",
    "\\end{bmatrix}$\n",
    "where each entry is 1 or 0 depending on whether the example belongs to that class.\n",
    "\n",
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (4, 150)\n",
      "Shape of y: (3, 150)\n",
      "\n",
      "5 columns of X:\n",
      "[[6.2 6.2 4.4 7.9 7.9]\n",
      " [2.2 2.8 3.  3.8 3.8]\n",
      " [4.5 4.8 1.3 6.4 6.4]\n",
      " [1.5 1.8 0.2 2.  2. ]]\n",
      "5 corresponding columns of y:\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "# from numpy import loadtxt\n",
    "\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data'\n",
    "raw_data = str(urlopen(url).read())[2:-5]\n",
    "rows = str(raw_data).split(\"\\\\n\")\n",
    "\n",
    "X = np.array([np.array(r.split(\",\")[:-1]) for r in rows]).astype(float)\n",
    "X = X.T\n",
    "\n",
    "y = np.zeros((150,3))\n",
    "y[:50,0] = 1\n",
    "y[50:100,1] = 1\n",
    "y[100:150,2] = 1\n",
    "y = y.T\n",
    "\n",
    "# make sure it loaded correctly\n",
    "load_sample = (np.random.rand(5)*150).astype(int)\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print()\n",
    "\n",
    "print(\"5 columns of X:\")\n",
    "print(X[:,load_sample])\n",
    "\n",
    "print(\"5 corresponding columns of y:\")\n",
    "print(y[:,load_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each column in the 5 columns of $y$ has exactly one entry of 1, and the rest are 0. When we predict $\\hat{y}$, we want the prediction for the true class to be close to 1, and the predictions for the other classes to be close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Two weight matrices (`W1` and `W2`) and two biases (`b1` and `b2`) need to be initialized. Their sizes depend on the number of entries in each example (`n_x`), the number of hidden nodes (`n_h`) and the number of classes (`n_y`).\n",
    "\n",
    "Each layer of the network transforms the input vector into a different vector space of some number of dimensions.\n",
    "The size of `W1` is `n_h` by `n_x`, to transform an `n_x`-dimensional input vector into an `n_h`-dimensional vector in the hidden layer. `b1` is size `n_h`, and is added to the hidden layer vector as bias.\n",
    "The size of `W2` is `n_y` by `n_h`, to transform an `n_h`-dimensional hidden layer vector into an `n_y`-dimensional vector representing class probabilities. Bias vector `b2` is size `n_y`.\n",
    "\n",
    "Below, fill in the function to set the size of the input, hidden layer, and output, initialize the weights with small random values, and initialize the biases to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    '''\n",
    "    Input\n",
    "    -----\n",
    "    n_x: size of example input vector\n",
    "    n_h: number of nodes in the hidden layer\n",
    "    n_y: number of classes\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    parameters: dictionary of initialized weights\n",
    "    '''\n",
    "    # hint: look up np.random.randn and np.zeros\n",
    "    W1 = \n",
    "    b1 = \n",
    "    W2 = \n",
    "    b2 = \n",
    "    \n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propogation\n",
    "\n",
    "Forward propogation computes the probabilities of a certain example belonging to each class. Each prediction is between 0 and 1 (the range of the `sigmoid` function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the math for moving one example through the network:\n",
    "\n",
    "* Input is $x$ ($x$ is a `n_x`-d column vector)\n",
    "* First transformation, $W_1 \\cdot x + b_1 = z_1$ ($z_1$ is a `n_h`-d vector)\n",
    "* Apply activation function, $a_1 = \\textrm{tanh}(z_1)$ (dimension stays the same)\n",
    "* Second transformation, $W_2 \\cdot a_1 + b_2 = z_2$ ($z_2$ is a `n_y`-d vector)\n",
    "* Apply activation function, $a_2 = \\textrm{sigmoid}(z_2)$ (dimension stays the same)\n",
    "* The predicted class scores are $\\hat{y} = a_2$\n",
    "\n",
    "\n",
    "Doing this separately for every single example is slow, so we use vectorization to process multiple examples at once. If we process `n` examples, the input $x$ becomes $X$, a `n_x` by `n` matrix, and $\\hat{y}$ becomes a `n_y` by `n` matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the forward pass of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "def forward_propogation(X, parameters):\n",
    "    '''\n",
    "    Input\n",
    "    -----\n",
    "    X: array of input examples (shape: num_measurements x num_examples)\n",
    "    parameters: dictionary of weights\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    y_hat: probabilities of predicted classes (shape: num_classes x num_examples)\n",
    "    cache: intermediate values (used in backpropogation)\n",
    "    '''\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = \n",
    "    A1 = \n",
    "    Z2 = \n",
    "    y_hat = A2 =  # y_hat are the predicted scores, 0 < score < 1, for each of the classes\n",
    "    \n",
    "    # save intermediate values for backpropogation\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"y_hat\": y_hat}\n",
    "    \n",
    "    return y_hat, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost\n",
    "\n",
    "We need some way to know how well our neural network is performing. We define a loss function to compute how wrong each output of the network was. The cost is the average loss over all the training examples.\n",
    "\n",
    "Here, we will use the logarithmic loss function. \n",
    "\n",
    "For each individual class prediction (e.g. for 100 examples with 4 classes that would be 400 individual predictions when we predict the probability of each example being in each class), if the true value for a particular class prediction is 0 (e.g. an Iris virginica would be labeled with a 0 for the Iris setosa class), the loss is $-log(1-\\hat{y})$, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: could not find rc file; returning defaults\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0,1,0.01)\n",
    "plt.plot(x, -np.log(1-x))\n",
    "plt.axis([0,1,0,4])\n",
    "plt.xlabel(\"y_hat\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the loss is high when the prediction $\\hat{y}$ is far from the correct value of 0.\n",
    "\n",
    "If the true value for a particular class prediction is 1 (e.g. an Iris versicolor would be labeled with a 1 for the Iris versicolor class), the loss is $-log(\\hat{y})$, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1e-4,1,0.01)\n",
    "plt.plot(x, -np.log(x))\n",
    "plt.axis([0,1,0,4])\n",
    "plt.xlabel(\"y_hat\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the loss is high when the prediction $\\hat{y}$ is far from the correct value of 1.\n",
    "\n",
    "The loss for one example is the sum of the losses over each class, and the total cost is the average of the losses for each example.\n",
    "\n",
    "To sum all of these efficiently, we can exploit the fact that the labels are `1`s and `0`s.\n",
    "We can add all the losses for one example $i$ using the equation \n",
    "\n",
    "$$L_i = {\\sum_{j \\in \\textrm{classes}}} -y_j\\cdot \\log(\\hat{y_j}) -(1-y_j)\\cdot \\log(1-\\hat{y}_j)$$\n",
    "\n",
    "where $y_j$ is the label for class $j$ of example $i$ (with a value of 1 or 0) and $\\hat{y}_j$ is the prediction.\n",
    "\n",
    "Note that if the label for class $j$ is 1, then $-y_j\\cdot \\log(\\hat{y_j}) -(1-y_j)\\cdot \\log(1-\\hat{y}_j)$ is the same as $-y_j\\cdot \\log(\\hat{y_j})$, and if the label is 0, it is $-(1-y_j)\\cdot \\log(1-\\hat{y}_j)$. So everything is added correctly.\n",
    "\n",
    "The total cost for $m$ examples is $J = \\frac{1}{m} {\\sum_{i=1}^m} L_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the cost function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y_hat, y):\n",
    "    '''\n",
    "    Input\n",
    "    -----\n",
    "    y_hat: computed class scores\n",
    "    y: true classes\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    loss: total loss (scalar)\n",
    "    '''\n",
    "    num_examples = y.shape[0]\n",
    "    \n",
    "    logloss = \n",
    "    cost = - np.sum(logloss) / num_examples\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propogation\n",
    "\n",
    "\n",
    "We want to minimize the cost. To do so, we compute the gradient of the loss with respect to each of our weights ($W_1$, $b_1$, $W_2$, $b_2$). The gradient is the direction of greatest increase of the cost, so we subtract the gradient times a learning rate from all the weights to make the cost smaller.\n",
    "\n",
    "Remember from before,\n",
    "\n",
    "* Input is $x$\n",
    "* $W_1 \\cdot x + b_1 = z_1$\n",
    "* $a_1 = \\textrm{tanh}(z_1)$\n",
    "* $W_2 \\cdot a_1 + b_2 = z_2$\n",
    "* $a_2 = \\textrm{sigmoid}(z_2)$\n",
    "* The predicted class scores are $\\hat{y} = a_2$\n",
    "\n",
    "and then we compute the losses $L_i = {\\sum_{j \\in \\textrm{classes}}} -y_j\\cdot \\log(\\hat{y_j}) -(1-y_j)\\cdot \\log(1-\\hat{y}_j)$\n",
    "and the total cost $J = \\frac{1}{m} {\\sum_{i=1}^{m}} L_i$.\n",
    "\n",
    "We need to find the [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative) of $J$ with respect to each of $W_1$, $b_1$, $W_2$, and $b_2$.\n",
    "We do this by moving backwards through the process:\n",
    "\n",
    "Intermediate derivative: $\\frac{\\partial J}{\\partial L_i} \\cdot \\frac{\\partial L_i}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} = \\frac{\\partial J}{\\partial z_2} = \\frac{1}{m} {\\sum_j} (\\hat{y}_j - y_j)$\n",
    "\n",
    "Find $\\frac{\\partial J}{\\partial W_2}$: $\\frac{\\partial J}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial W_2} = \\frac{1}{m} {\\sum_j} (\\hat{y}_j - y_j) \\cdot a_1 = \\frac{\\partial J}{\\partial W_2}$\n",
    "\n",
    "Find $\\frac{\\partial J}{\\partial b_2}$: $\\frac{\\partial J}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial b_2} = \\frac{1}{m} {\\sum_j} (\\hat{y}_j - y_j) \\cdot 1 = \\frac{\\partial J}{\\partial b_2}$\n",
    "\n",
    "Intermediate derivative: $\\frac{\\partial z_2}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} = W_2 \\cdot (1 - a_1^2)$\n",
    "\n",
    "Find $\\frac{\\partial J}{\\partial W_1}$: $\\frac{\\partial J}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial W_1} = \\frac{1}{m} {\\sum_j} (\\hat{y}_j - y_j) \\cdot W_2 \\cdot (1 - a_1^2) \\cdot x $\n",
    "\n",
    "Find $\\frac{\\partial J}{\\partial b_1}$: $\\frac{\\partial J}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1} = \\frac{1}{m} {\\sum_j} (\\hat{y}_j - y_j) \\cdot W_2 \\cdot (1 - a_1^2) \\cdot 1$\n",
    "\n",
    "Make sure to account for dimensions when you implement it in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement backpropogation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(parameters, cache, X, y):\n",
    "    '''\n",
    "    Input\n",
    "    -----\n",
    "    parameters: dictionary of weights\n",
    "    cache: intermediate variables from most recent computation\n",
    "    X: array of input examples\n",
    "    y: true classes\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    grads: dictionary of gradients\n",
    "    '''\n",
    "    num_examples = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = cache[\"Z1\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    Z2 = cache[\"Z2\"]\n",
    "    y_hat = cache[\"y_hat\"]\n",
    "    \n",
    "    dZ2 = \n",
    "    dW2 = \n",
    "    db2 = \n",
    "    dZ1 = \n",
    "    dW1 = \n",
    "    db1 = \n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights\n",
    "\n",
    "Subtract the gradients times the learning rate from the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(parameters, grads, learning_rate=0.001):\n",
    "    '''\n",
    "    Input\n",
    "    -----\n",
    "    parameters: dictionary of weights\n",
    "    grads: dicitonary of gradients\n",
    "    learning_rate: learning rate\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    parameters: dictionary of updated weights\n",
    "    ''' \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = \n",
    "    b1 = \n",
    "    W2 = \n",
    "    b2 = \n",
    "    \n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, y, n_h, num_iterations=50000):\n",
    "    '''\n",
    "    Input\n",
    "    -----\n",
    "    X: array of input examples\n",
    "    y: true classes\n",
    "    n_h: number of hidden nodes\n",
    "    num_iterations: number of times to update the weights\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    parameters: the weights of the trained model\n",
    "    '''\n",
    "    n_x = X.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        y_hat, cache =  # do forward propogation\n",
    "        cost =  # compute the cost\n",
    "        costs.append(cost)\n",
    "        grads =  # do backpropogation\n",
    "        parameters =  # update the parameters\n",
    "        \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "Set aside some of the data to evaluate the model later. We want to evaluate the model with data it's never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (4, 105)\n",
      "Shape of y_train: (3, 105)\n",
      "Shape of X_test: (4, 76)\n",
      "Shape of y_test: (3, 76)\n"
     ]
    }
   ],
   "source": [
    "train = np.random.choice(range(X.shape[1]), int(0.7*X.shape[1]))\n",
    "test = [i for i in range(X.shape[1]) if i not in train]\n",
    "\n",
    "X_train = X[:,train]\n",
    "X_test = X[:,test]\n",
    "y_train = y[:,train]\n",
    "y_test = y[:,test]\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll have 4 nodes in the hidden layer\n",
    "n_h = 4\n",
    "\n",
    "params, costs = nn_model(X_train, y_train, n_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, _ = forward_propogation(X_test, params)\n",
    "\n",
    "predicted = y_hat.argmax(0)\n",
    "y_test_classes = y_test.argmax(0)\n",
    "\n",
    "acc = np.sum(y_test == predicted) / float(y_test.shape[1])\n",
    "\n",
    "print(\"Percent correct: %f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Loss and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,50000,1)\n",
    "plt.plot(x, costs)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvements / study\n",
    "\n",
    "Some other things you could look into:\n",
    "* How does changing the number of nodes in the hidden layer affect training and accuracy?\n",
    "* Overfittting?\n",
    "* Regularization?\n",
    "* Cross-validation?\n",
    "\n",
    "Also note that you can rearrange the linear algebra equations a bit and still arrive ate the same answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
